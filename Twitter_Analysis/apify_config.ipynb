{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apify_client in c:\\users\\sreej\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.3)\n",
      "Requirement already satisfied: apify-shared~=1.1.1 in c:\\users\\sreej\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from apify_client) (1.1.1)\n",
      "Requirement already satisfied: httpx>=0.25.1 in c:\\users\\sreej\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from apify_client) (0.27.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\sreej\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.25.1->apify_client) (4.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\sreej\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.25.1->apify_client) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sreej\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.25.1->apify_client) (1.0.4)\n",
      "Requirement already satisfied: idna in c:\\users\\sreej\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.25.1->apify_client) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sreej\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.25.1->apify_client) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sreej\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.1->apify_client) (0.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install apify_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from apify_client import ApifyClient\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Function to load apify_setup from a JSON file\n",
    "def load_apify_setup(file_name):\n",
    "    try:\n",
    "        with open(file_name, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading configuration file {file_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to check whether tweets contain search keywords\n",
    "def contains_keyword(text, search_keyword):\n",
    "    if search_keyword.lower() in text.lower():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Function to fetch relevant tweets\n",
    "def run_actor(client, setup):\n",
    "    try:\n",
    "        # Get the current date\n",
    "        current_date = datetime.now()\n",
    "\n",
    "        # Initiate the datetime variable\n",
    "        since_date = datetime.now()\n",
    "\n",
    "        if setup[\"frequency\"] == 'daily':\n",
    "            # Go back by a day\n",
    "            since_date = current_date - timedelta(days=1)\n",
    "        elif setup[\"frequency\"] == 'weekly':\n",
    "            # Go back by a week\n",
    "            since_date = current_date - timedelta(weeks=1)\n",
    "        elif setup[\"frequency\"] == 'monthly':\n",
    "            # Go back by a month\n",
    "            since_date = current_date - timedelta(days=30)\n",
    "\n",
    "        # Prepare the Actor input\n",
    "        run_input = {\n",
    "            \"searchTerms\": setup[\"search_keywords\"],\n",
    "            \"searchMode\": \"live\",\n",
    "            \"maxTweets\": setup[\"max_limit\"],\n",
    "            \"addUserInfo\": True,\n",
    "            \"scrapeTweetReplies\": False,\n",
    "            \"sinceDate\": since_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"untilDate\": current_date.strftime(\"%Y-%m-%d\")\n",
    "        }\n",
    "\n",
    "        return client.actor(setup['actor_id']).call(run_input=run_input)\n",
    "    except Exception as e:\n",
    "        print(f\"Error searching tweets: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to group tweets by week based on their creation date.\n",
    "def group_tweets_by_week(tweets, search_keyword, tweet_fields, created_at_format):\n",
    "    \"\"\"\n",
    "    Groups tweets by week based on their creation date.\n",
    "\n",
    "    Args:\n",
    "        tweets (list): List of tweets to be grouped.\n",
    "        search_keyword (str): The keyword to filter tweets.\n",
    "        tweet_fields (dict): Dictionary containing tweet field mappings.\n",
    "        created_at_format (str): The format of the 'created_at' field in tweets.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are week ranges and values are lists of tweets.\n",
    "              Tweets within each week group are sorted by their creation date (latest first).\n",
    "              The dictionary is sorted by the start date of each week group (latest first).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tweets_by_week = {}\n",
    "        seen_ids = set()  # Set to keep track of encountered tweet IDs \n",
    "\n",
    "        for tweet in tweets:\n",
    "            if contains_keyword(tweet[tweet_fields[\"text\"]], search_keyword):\n",
    "                tweet_id = tweet[tweet_fields[\"tweet_id\"]]\n",
    "                # Skip if tweet ID is already encountered\n",
    "                if tweet_id in seen_ids:\n",
    "                    continue\n",
    "\n",
    "                seen_ids.add(tweet_id)\n",
    "                created_at = datetime.strptime(tweet[tweet_fields[\"created_at\"]], created_at_format)\n",
    "                # Calculate the start and end of the week for the tweet\n",
    "                start_of_week = created_at - timedelta(days=created_at.weekday())\n",
    "                end_of_week = start_of_week + timedelta(days=6)\n",
    "                week_key = f\"{start_of_week.strftime('%Y-%m-%d')}_to_{end_of_week.strftime('%Y-%m-%d')}\"\n",
    "\n",
    "                # Initialize an empty list for the week if not exists\n",
    "                if week_key not in tweets_by_week:\n",
    "                    tweets_by_week[week_key] = []\n",
    "                \n",
    "                # Dictionary to store processed tweet fields\n",
    "                tweet_json = {}\n",
    "\n",
    "                for key, value in tweet_fields.items():\n",
    "                    if key == \"public_metrics\":\n",
    "                        \n",
    "                        public_metrics_json = {} # Dictionary to store public metrics\n",
    "\n",
    "                        for item_key, item_value in tweet_fields[key].items():\n",
    "                            # Get public metric value from tweet\n",
    "                            public_metrics_json[item_key] = tweet.get(item_value, \"\")\n",
    "\n",
    "                        tweet_json[key] = public_metrics_json # Assign public metrics to tweet JSON\n",
    "                    elif key == 'created_at':\n",
    "                        tweet_json[key] = created_at.strftime('%Y-%m-%dT%H:%M:%S.000Z') # Format tweet creation date\n",
    "                    else:\n",
    "                        tweet_json[key] = tweet.get(value, \"\") # Get tweet field value from tweet\n",
    "\n",
    "                # Append processed tweet to the corresponding week group\n",
    "                tweets_by_week[week_key].append(tweet_json)\n",
    "\n",
    "        if tweets_by_week:\n",
    "            # Sort tweets within each week group by their creation date (latest first)\n",
    "            for week_key in tweets_by_week:\n",
    "                tweets_by_week[week_key] = sorted(tweets_by_week[week_key], key=lambda x: x[\"created_at\"], reverse=True)\n",
    "            \n",
    "            # Sort the groups by the start date of the week (latest first)\n",
    "            tweets_by_week = dict(sorted(tweets_by_week.items(), key=lambda x: datetime.strptime(x[0][:10], '%Y-%m-%d'), reverse=True))\n",
    "        \n",
    "        return tweets_by_week\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error organising {search_keyword} tweets: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to save a list of tweets to a generic and year-specific JSON files\n",
    "def organise_tweets_to_json(tweets, setup):\n",
    "    try:\n",
    "        search_keywords = setup[\"search_keywords\"]\n",
    "        tweet_fields = setup[\"tweet_fields\"]\n",
    "        created_at_format = setup[\"created_at_format\"]\n",
    "        base_directory = setup[\"base_directory\"]\n",
    "        count = 0\n",
    "\n",
    "        # Ensure the base directory exists\n",
    "        if not os.path.exists(base_directory):\n",
    "            os.makedirs(base_directory)\n",
    "\n",
    "        for keyword in search_keywords:\n",
    "            tweets_by_week = group_tweets_by_week(tweets, keyword, tweet_fields, created_at_format)\n",
    "\n",
    "            if tweets_by_week:\n",
    "                # Write grouped and sorted tweets to a JSON file\n",
    "                output_file = os.path.join(base_directory, f\"tweets_{keyword}.json\")\n",
    "\n",
    "                # Check if the file exists, and determine the mode accordingly\n",
    "                mode = 'r+' if os.path.isfile(output_file) else 'w'\n",
    "\n",
    "                # Open the file in the determined mode\n",
    "                with open(output_file, mode, encoding='utf-8') as file:\n",
    "\n",
    "                    # Initialize the list with existing data if exists\n",
    "                    existing_data = json.load(file) if mode == 'r+' else {}\n",
    "                    # Initialize a dictionary to store the tweets\n",
    "                    all_data = {**tweets_by_week, **existing_data}\n",
    "\n",
    "                    if mode == 'r+':\n",
    "                        # Reset the pointer if mode is 'r+'\n",
    "                        file.seek(0)\n",
    "\n",
    "                    # Write the data to the file\n",
    "                    json.dump(all_data, file, ensure_ascii=False, indent=4)\n",
    "                    logging.info(f\"{keyword} tweets saved to {output_file}.\")\n",
    "                    count += 1\n",
    "            else:\n",
    "                logging.warning(f\"No tweets found for {keyword}.\")\n",
    "\n",
    "        return count > 0\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing and saving tweets: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Set up Apify client\n",
    "        setup = load_apify_setup(\"apify_setup.json\")\n",
    "        if not setup:\n",
    "            logging.error(\"Error loading the set up.\")\n",
    "            return\n",
    "\n",
    "        # Initialize the ApifyClient with the API token\n",
    "        client = ApifyClient(setup[\"api_token\"])\n",
    "        if not client:\n",
    "            logging.error(\"Error initializing ApifyClient.\")\n",
    "            return\n",
    "\n",
    "        # Run the Actor and wait for it to finish\n",
    "        run = run_actor(client, setup)\n",
    "        if not run:\n",
    "            logging.error(\"Error running the actor.\")\n",
    "            return\n",
    "\n",
    "        response_generator = client.dataset(run[\"defaultDatasetId\"]).iterate_items()\n",
    "        if not response_generator:\n",
    "            logging.warning(\"Empty response.\")\n",
    "            return\n",
    "\n",
    "        tweets = list(response_generator)\n",
    "        if not tweets:\n",
    "            logging.warning(\"No tweets found.\")\n",
    "            return\n",
    "\n",
    "        # Process the tweets if the list is not empty\n",
    "        is_success = organise_tweets_to_json(tweets, setup)\n",
    "        if is_success:\n",
    "            logging.info(\"Request Executed Successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main execution: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
